{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiaratatyanadewi/UNGP_Chatbot/blob/main/REVISED_INTEGRATED_RAG_GEN_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXDSqtIMHwT4"
      },
      "outputs": [],
      "source": [
        "# Workaround to avoid following error at notebook\n",
        "# NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code to mount Google Drive at Colab Notebook instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjGQWiwaIBpZ",
        "outputId": "12879250-73fe-4632-b591-6891320e6718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface libraries to run LLM.\n",
        "!pip install -q -U transformers==4.40.2\n",
        "!pip install -q -U accelerate==0.30.1\n",
        "!pip install -q -U bitsandbytes==0.43.1\n",
        "!pip install -q -U huggingface_hub==0.23.0\n",
        "\n",
        "#LangChain related libraries\n",
        "!pip install -q -U langchain==0.1.2\n",
        "\n",
        "#Open-source pure-python PDF library capable of splitting, merging, cropping,\n",
        "#and transforming the pages of PDF files\n",
        "!pip install -q -U pypdf==4.2.0\n",
        "\n",
        "#Python framework for state-of-the-art sentence, text and image embeddings.\n",
        "!pip install -q -U sentence-transformers==2.7.0\n",
        "\n",
        "# FAISS Vector Databses specific Libraries\n",
        "!pip install -q -U faiss-gpu==1.7.2"
      ],
      "metadata": {
        "id": "QF1rUQtAH6sG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbebaa1e-d89b-49b8-aaac-36ed339679fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.5/238.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from typing import List\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Device:\", device)\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "xqGJmZ6iIwz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40121a52-b256-4191-f200-ae72e48104ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Set up your API key\n",
        "api_key = input(\"Please enter your Google Generative AI API key: \")\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "YjVMrPDZI0_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# Folder path yang berisi file CSV\n",
        "folder_path = '/content/drive/MyDrive/UNGP'\n",
        "\n",
        "all_data = []\n",
        "\n",
        "# Iterasi melalui semua file dalam folder dan memuat CSV file\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        loader = CSVLoader(file_path=file_path)\n",
        "        data = loader.load()\n",
        "        all_data.extend(data)"
      ],
      "metadata": {
        "id": "XqXvhMgeQQ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search function to find relevant data in the CSV files\n",
        "def search_csv(query, data):\n",
        "    results = []\n",
        "    for row in data:\n",
        "        if any(query.lower() in str(value).lower() for value in row.values()):\n",
        "            results.append(row)\n",
        "    return results\n",
        "\n",
        "# Set up the HuggingFace embeddings and FAISS retriever\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "chunked_docs = text_splitter.split_documents(all_data)\n"
      ],
      "metadata": {
        "id": "LVUaaTjXQb8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "DvDSDZVO1fSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "db = FAISS.from_documents(chunked_docs, embeddings)\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 4})\n"
      ],
      "metadata": {
        "id": "y7u4JfEaYmC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Load the Mistral model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "hf_token = input(\"Please enter your Hugging Face token: \")\n",
        "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=hf_token\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_auth_token=hf_token)\n"
      ],
      "metadata": {
        "id": "0qTUmvgPP_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.3,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
      ],
      "metadata": {
        "id": "atno7vYz1rQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Conversational Retrieval Chain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(mistral_llm, retriever, return_source_documents=True)"
      ],
      "metadata": {
        "id": "d2Pe4dqSQ1Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a response\n",
        "def generate_response(query, data):\n",
        "    matched_data = search_csv(query, data)\n",
        "\n",
        "    if matched_data:\n",
        "        matched_text = \"\\n\".join([str(row) for row in matched_data])\n",
        "        prompt = f\"The user asked: '{query}'. Here is the related information from the CSV data:\\n\\n{matched_text}\\n\\nPlease provide a summary or answer.\"\n",
        "    else:\n",
        "        prompt = f\"The user asked: '{query}', but no relevant information was found in the data.\"\n",
        "\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "    chat_session = model.start_chat(history=[])\n",
        "    response = chat_session.send_message(prompt)\n",
        "\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "IicHty041zul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "l7m-P_cArwzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_interface(query):\n",
        "    # Retrieve response from Mistral-based QA chain\n",
        "    result = qa_chain.invoke({'question': query, 'chat_history': []})\n",
        "\n",
        "    mistral_answer = result['answer']\n",
        "    # Check if Mistral's answer is inadequate (e.g., \"I don't know.\")\n",
        "    if mistral_answer.strip().lower() in [\"i don't know.\", \"no relevant information\", \"not found\"]:\n",
        "        # Fallback to Gemini if Mistral doesn't find an answer\n",
        "        prompt = f\"The user asked: '{query}', but the dataset did not provide sufficient information. Please generate a response.\"\n",
        "        model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
        "        chat_session = model.start_chat(history=[])\n",
        "        response = chat_session.send_message(prompt)\n",
        "\n",
        "        return f\"{response.text}\\n\\n(Note: This response was generated using Gemini AI Model.)\"\n",
        "    else:\n",
        "        # If Mistral provides an answer, use it\n",
        "        sources = [doc.metadata['source'] for doc in result['source_documents']]\n",
        "        source_files = ', '.join(sources)\n",
        "        answer_with_sources = f\"{mistral_answer}\\n\\nSource files: {source_files}\"\n",
        "        return answer_with_sources\n"
      ],
      "metadata": {
        "id": "4VTS_ofvCS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Gradio interface setup\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interface,\n",
        "    inputs=gr.Textbox(label=\"Input\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"UNGP DEMO CHATBOT\",\n",
        "    description=\"Ask questions about the data and get responses using the Gemini API and Mistral model.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "id": "mYRwkfW9h3Ud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}